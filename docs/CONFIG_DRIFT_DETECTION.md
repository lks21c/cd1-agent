# Drift Agent - AWS Configuration Drift Detection

> **ì„œë¸Œ ì—ì´ì „íŠ¸**: Drift Agent (ë³€ê²½ê´€ë¦¬ Agent)
>
> ë¡œì»¬ ê¸°ì¤€ì„  ëŒ€ë¹„ AWS ë¦¬ì†ŒìŠ¤ êµ¬ì„± ë“œë¦¬í”„íŠ¸ íƒì§€ ì‹œìŠ¤í…œ.

## ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
3. [LLM ê¸°ë°˜ ì›ì¸ ë¶„ì„](#llm-ê¸°ë°˜-ì›ì¸-ë¶„ì„)
4. [Human-in-the-Loop (HITL) í†µí•©](#human-in-the-loop-hitl-í†µí•©)
5. [ë“œë¦¬í”„íŠ¸ íƒì§€ ì•Œê³ ë¦¬ì¦˜](#ë“œë¦¬í”„íŠ¸-íƒì§€-ì•Œê³ ë¦¬ì¦˜)
6. [ê¸°ì¤€ì„  ê´€ë¦¬](#ê¸°ì¤€ì„ -ê´€ë¦¬)
7. [ì§€ì› ë¦¬ì†ŒìŠ¤](#ì§€ì›-ë¦¬ì†ŒìŠ¤)
8. [í™˜ê²½ ë³€ìˆ˜](#í™˜ê²½-ë³€ìˆ˜)
9. [DynamoDB í…Œì´ë¸”](#dynamodb-í…Œì´ë¸”)
10. [EventBridge ì´ë²¤íŠ¸](#eventbridge-ì´ë²¤íŠ¸)
11. [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
12. [Mock í…ŒìŠ¤íŠ¸](#mock-í…ŒìŠ¤íŠ¸)

---

## ê°œìš”

CD1 Agentì˜ êµ¬ì„± ë“œë¦¬í”„íŠ¸ íƒì§€ ëª¨ë“ˆì€ AWS ê´€ë¦¬í˜• ì„œë¹„ìŠ¤(EKS, MSK, S3, EMR, MWAA)ì˜ í˜„ì¬ êµ¬ì„±ì„ ë¡œì»¬ `conf/baselines/` ë””ë ‰í† ë¦¬ì— ì €ì¥ëœ JSON ê¸°ì¤€ì„  íŒŒì¼ê³¼ ë¹„êµí•˜ì—¬ ì˜ë„ì¹˜ ì•Šì€ êµ¬ì„± ë³€ê²½ì„ ê°ì§€í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- **ë¡œì»¬ ê¸°ì¤€ì„  ê´€ë¦¬**: JSON í˜•ì‹ì˜ êµ¬ì„± ê¸°ì¤€ì„  íŒŒì¼ì„ agentì˜ `conf/baselines/` ë””ë ‰í† ë¦¬ì—ì„œ ê´€ë¦¬
- **ë‹¤ì¤‘ ë¦¬ì†ŒìŠ¤ ì§€ì›**: EKS, MSK, S3, EMR, MWAA êµ¬ì„± ë“œë¦¬í”„íŠ¸ íƒì§€
- **Cross-Account ì§€ì›**: AssumeRoleì„ í†µí•œ ë‹¤ì¤‘ ê³„ì • êµ¬ì„± ì¡°íšŒ
- **í•„ë“œ ë ˆë²¨ ë¹„êµ**: JSON Diff ê¸°ë°˜ì˜ ì„¸ë¶„í™”ëœ ë“œë¦¬í”„íŠ¸ ê°ì§€
- **ì‹¬ê°ë„ ë¶„ë¥˜**: ë³´ì•ˆ/ìŠ¤í™/ìŠ¤ì¼€ì¼ë§/ë©”íƒ€ë°ì´í„° ë³€ê²½ì— ë”°ë¥¸ ì‹¬ê°ë„ ìë™ ë¶„ë¥˜
- **ìë™ ì•Œë¦¼**: EventBridgeë¥¼ í†µí•œ ë“œë¦¬í”„íŠ¸ ì•Œë¦¼
- **ì´ë ¥ ê´€ë¦¬**: DynamoDB ê¸°ë°˜ ë“œë¦¬í”„íŠ¸ ì´ë ¥ ì¶”ì 

### ì‚¬ìš© ì‚¬ë¡€

1. **Compliance ëª¨ë‹ˆí„°ë§**: í”„ë¡œë•ì…˜ í™˜ê²½ì˜ êµ¬ì„±ì´ ìŠ¹ì¸ëœ ê¸°ì¤€ì„ ì—ì„œ ë²—ì–´ë‚¬ëŠ”ì§€ ì§€ì† í™•ì¸
2. **ë³€ê²½ ê°ì§€**: ìˆ˜ë™ ë³€ê²½ì´ë‚˜ ìë™í™” ì˜¤ë¥˜ë¡œ ì¸í•œ ì˜ë„ì¹˜ ì•Šì€ êµ¬ì„± ë³€ê²½ íƒì§€
3. **ë³´ì•ˆ ê°ì‚¬**: ë³´ì•ˆ ê´€ë ¨ ì„¤ì •(ì•”í˜¸í™”, í¼ë¸”ë¦­ ì•¡ì„¸ìŠ¤ ë“±) ë³€ê²½ ì¦‰ì‹œ ì•Œë¦¼
4. **ìš©ëŸ‰ ê³„íš**: ë¦¬ì†ŒìŠ¤ ìŠ¤í™ ë³€ê²½ ì¶”ì  ë° ë¹„ìš© ì˜í–¥ ë¶„ì„

---

## ì•„í‚¤í…ì²˜

```mermaid
flowchart TB
    subgraph trigger["MWAA (Daily Trigger)"]
        mwaa["Scheduled Trigger"]
    end

    subgraph detection["Drift Detection Phase"]
        subgraph local["Local Baselines (conf/)"]
            baselines["conf/baselines/<br/>â”œâ”€â”€ eks/<br/>â”œâ”€â”€ msk/<br/>â”œâ”€â”€ s3/<br/>â”œâ”€â”€ emr/<br/>â””â”€â”€ mwaa/"]
        end

        subgraph aws["AWS Managed Services"]
            eks["EKS"]
            msk["MSK"]
            s3["S3"]
            emr["EMR"]
            mwaaService["MWAA"]
        end

        subgraph lambda["Lambda: bdp-drift-detection"]
            baselineLoader["Baseline<br/>Loader"]
            configFetcher["Config<br/>Fetcher"]
            driftDetector["Drift<br/>Detector"]
            driftAnalyzer["Drift<br/>Analyzer"]
            alertPublisher["Alert<br/>Publisher"]

            baselineLoader --> configFetcher --> driftDetector
            driftDetector -->|"CRITICAL/HIGH"| driftAnalyzer
            driftDetector -->|"MEDIUM/LOW"| alertPublisher
            driftAnalyzer --> alertPublisher
        end

        baselines -->|"File Read"| baselineLoader
        eks & msk & s3 & emr & mwaaService -->|"AWS Describe APIs"| configFetcher
    end

    subgraph llm["LLM Service"]
        vllm["vLLM / Gemini"]
    end

    subgraph outputs["Outputs"]
        dynamodb["DynamoDB<br/>(Store + Analysis)"]
        eventbridge["EventBridge<br/>(Alert + Analysis)"]
    end

    mwaa --> detection
    driftAnalyzer <-->|"ReAct Loop"| vllm
    alertPublisher --> dynamodb & eventbridge
```

### LLM ê¸°ë°˜ ë¶„ì„ ìƒì„¸ í”Œë¡œìš°

```mermaid
flowchart LR
    subgraph input["Input"]
        drift["Detected Drift<br/>(CRITICAL/HIGH)"]
        baseline["Baseline Config"]
        current["Current Config"]
    end

    subgraph react["ReAct Analysis Loop"]
        plan["ğŸ¯ PLAN<br/>ë¶„ì„ ê³„íš ìˆ˜ë¦½"]
        analyze["ğŸ” ANALYZE<br/>LLM ì›ì¸ ë¶„ì„"]
        reflect["ğŸª REFLECT<br/>í’ˆì§ˆ í‰ê°€"]

        plan --> analyze --> reflect
        reflect -->|"confidence < 0.7<br/>needs_replan"| plan
    end

    subgraph llm["LLM"]
        model["vLLM / Gemini<br/>Structured Output"]
    end

    subgraph output["Output"]
        result["DriftAnalysisResult"]
        cause["ì›ì¸ ì¹´í…Œê³ ë¦¬<br/>MANUAL_CHANGE<br/>AUTO_SCALING<br/>DEPLOYMENT_DRIFT<br/>..."]
        remediation["ì¡°ì¹˜ ê¶Œê³ <br/>revert_to_baseline<br/>update_baseline<br/>escalate"]
    end

    drift & baseline & current --> plan
    analyze <--> model
    reflect <--> model
    reflect -->|"confidence â‰¥ 0.7<br/>complete"| result
    result --> cause & remediation
```

### ì»´í¬ë„ŒíŠ¸

| ì»´í¬ë„ŒíŠ¸ | íŒŒì¼ | ì„¤ëª… |
|---------|------|------|
| Baseline Loader | `src/agents/drift/services/baseline_loader.py` | ë¡œì»¬ ê¸°ì¤€ì„  íŒŒì¼ ë¡œë“œ |
| Config Fetcher | `src/agents/drift/services/config_fetcher.py` | AWS Describe API ì¶”ìƒí™” |
| Drift Detector | `src/agents/drift/services/drift_detector.py` | JSON Diff ê¸°ë°˜ ë“œë¦¬í”„íŠ¸ íƒì§€ ì—”ì§„ |
| Drift Detection Handler | `src/agents/drift/handler.py` | Lambda í•¸ë“¤ëŸ¬ |
| **Drift Analyzer** | `src/agents/drift/services/drift_analyzer.py` | **LLM ê¸°ë°˜ ì›ì¸ ë¶„ì„ (ReAct)** |

---

## LLM ê¸°ë°˜ ì›ì¸ ë¶„ì„

### ê°œìš”

ë“œë¦¬í”„íŠ¸ íƒì§€ í›„ CRITICAL/HIGH ì‹¬ê°ë„ì˜ ë“œë¦¬í”„íŠ¸ì— ëŒ€í•´ LLMì„ í†µí•œ ê·¼ë³¸ ì›ì¸ ë¶„ì„(Root Cause Analysis)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ReAct(Reasoning + Acting) íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ë¶„ì„ í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤.

### ReAct ë¶„ì„ í”Œë¡œìš°

```mermaid
flowchart TB
    subgraph detection["Drift Detection"]
        detect["ë“œë¦¬í”„íŠ¸ íƒì§€"]
        filter["CRITICAL/HIGH í•„í„°"]
    end

    subgraph react["ReAct Analysis Loop"]
        plan["PLAN<br/>ë¶„ì„ ê³„íš ìˆ˜ë¦½"]
        analyze["ANALYZE<br/>LLM ì›ì¸ ë¶„ì„"]
        reflect["REFLECT<br/>ë¶„ì„ í’ˆì§ˆ í‰ê°€"]

        plan --> analyze --> reflect
        reflect -->|"needs_replan<br/>(confidence < 0.7)"| plan
        reflect -->|"complete<br/>(confidence â‰¥ 0.7)"| result
    end

    subgraph output["Output"]
        result["ë¶„ì„ ê²°ê³¼"]
        store["DynamoDB ì €ì¥"]
        event["EventBridge ë°œí–‰"]
    end

    detect --> filter --> react
    result --> store & event
```

### ì›ì¸ ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬

| ì¹´í…Œê³ ë¦¬ | ì„¤ëª… | ì˜ˆì‹œ |
|---------|------|------|
| `MANUAL_CHANGE` | ì½˜ì†”/CLIë¥¼ í†µí•œ ìˆ˜ë™ ë³€ê²½ | AWS Consoleì—ì„œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë³€ê²½ |
| `AUTO_SCALING` | ASG/HPA ë“± ìë™ ìŠ¤ì¼€ì¼ë§ | EKS ë…¸ë“œê·¸ë£¹ desired_size ë³€ê²½ |
| `MAINTENANCE_WINDOW` | AWS ìë™ ìœ ì§€ë³´ìˆ˜ | RDS íŒ¨ì¹˜, ì—”ì§„ ì—…ê·¸ë ˆì´ë“œ |
| `DEPLOYMENT_DRIFT` | IaC ìƒíƒœ ë¶ˆì¼ì¹˜ | Terraform stateì™€ ì‹¤ì œ êµ¬ì„± ì°¨ì´ |
| `SECURITY_PATCH` | ìë™ ë³´ì•ˆ íŒ¨ì¹˜ | Lambda ëŸ°íƒ€ì„ ì—…ë°ì´íŠ¸ |
| `OPERATOR_ERROR` | ìš´ì˜ì ì‹¤ìˆ˜ | ì˜ëª»ëœ êµ¬ì„± ì ìš© |
| `UNKNOWN` | ì¦ê±° ë¶€ì¡± | ì›ì¸ íŒŒì•… ë¶ˆê°€ |

### ë¶„ì„ ê²°ê³¼ ëª¨ë¸

```python
class DriftAnalysisResult(BaseModel):
    """LLM ê¸°ë°˜ ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼."""

    drift_id: str                           # EKS:production-cluster
    resource_type: str                      # EKS
    resource_id: str                        # production-cluster

    # ì›ì¸ ë¶„ì„
    cause_analysis: DriftCauseAnalysis      # ì›ì¸ ì¹´í…Œê³ ë¦¬, ê·¼ë³¸ ì›ì¸, ì¦ê±°
    impact_assessment: str                  # ì˜í–¥ë„ í‰ê°€
    blast_radius: List[str]                 # ì˜í–¥ ë°›ëŠ” ì‹œìŠ¤í…œ ëª©ë¡

    # ì‹ ë¢°ë„ ì ìˆ˜
    confidence_score: float                 # 0.0-1.0 (0.7 ì´ìƒ ê¶Œì¥)
    urgency_score: float                    # ì¡°ì¹˜ ê¸´ê¸‰ë„

    # ì¡°ì¹˜ ê¶Œê³ 
    remediations: List[DriftRemediationAction]  # ì¡°ì¹˜ ë‹¨ê³„
    requires_human_review: bool             # ì‚¬ëŒ ê²€í†  í•„ìš” ì—¬ë¶€
    review_reason: Optional[str]            # ê²€í†  í•„ìš” ì‚¬ìœ 
```

### ì¡°ì¹˜ ê¶Œê³  íƒ€ì…

| íƒ€ì… | ì„¤ëª… | ì˜ˆì‹œ ëª…ë ¹ |
|-----|------|----------|
| `revert_to_baseline` | ê¸°ì¤€ì„ ìœ¼ë¡œ ë¡¤ë°± | `terraform apply -target=...` |
| `update_baseline` | ê¸°ì¤€ì„  ì—…ë°ì´íŠ¸ | Git commitìœ¼ë¡œ ê¸°ì¤€ì„  ê°±ì‹  |
| `escalate` | ìƒìœ„ ì—ìŠ¤ì»¬ë ˆì´ì…˜ | ë³´ì•ˆíŒ€ í†µë³´ |
| `investigate` | ì¶”ê°€ ì¡°ì‚¬ í•„ìš” | CloudTrail ë¡œê·¸ í™•ì¸ |
| `notify` | ì•Œë¦¼ë§Œ í•„ìš” | ê´€ë ¨ íŒ€ í†µë³´ |

### ReAct ë°˜ë³µ ì¡°ê±´

```python
# ë¶„ì„ ì™„ë£Œ ì¡°ê±´
while iteration <= MAX_ITERATIONS:
    analysis = analyze(drift)
    reflection = reflect(analysis)

    if reflection.overall_confidence >= 0.7:
        break  # ì™„ë£Œ

    if not reflection.needs_replan:
        break  # ì¬ë¶„ì„ ë¶ˆí•„ìš”

    iteration += 1  # ì¬ë¶„ì„
```

### ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ì¤€

| ì ìˆ˜ ë²”ìœ„ | ì˜ë¯¸ | ì¡°ì¹˜ |
|----------|------|------|
| 0.85+ | ëª…í™•í•œ ì¦ê±° | ìë™ ì¡°ì¹˜ ê°€ëŠ¥ |
| 0.70-0.84 | ê°•í•œ ê°€ì„¤ | ê²€í†  í›„ ì¡°ì¹˜ |
| 0.50-0.69 | í•©ë¦¬ì  ê°€ì„¤ | ì¶”ê°€ ì¡°ì‚¬ í•„ìš” |
| 0.50 ë¯¸ë§Œ | ì¦ê±° ë¶€ì¡± | ìˆ˜ë™ ë¶„ì„ ê¶Œì¥ |

### í™˜ê²½ ë³€ìˆ˜ (ë¶„ì„ ê´€ë ¨)

| ë³€ìˆ˜ëª… | ì„¤ëª… | ê¸°ë³¸ê°’ |
|--------|------|--------|
| `ENABLE_DRIFT_ANALYSIS` | LLM ë¶„ì„ í™œì„±í™” | `true` |
| `LLM_PROVIDER` | LLM ì œê³µì (vllm/gemini/mock) | `mock` |
| `VLLM_ENDPOINT` | vLLM ì„œë²„ ì—”ë“œí¬ì¸íŠ¸ | - |
| `LLM_MODEL` | ì‚¬ìš©í•  ëª¨ë¸ëª… | - |
| `MAX_ANALYSIS_ITERATIONS` | ReAct ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ | `3` |
| `ANALYSIS_CONFIDENCE_THRESHOLD` | ì™„ë£Œ ì‹ ë¢°ë„ ì„ê³„ê°’ | `0.7` |
| `MAX_DRIFTS_TO_ANALYZE` | ë¶„ì„í•  ìµœëŒ€ ë“œë¦¬í”„íŠ¸ ìˆ˜ | `5` |

### ë¶„ì„ ê²°ê³¼ ì €ì¥

DynamoDBì— ë¶„ì„ ê²°ê³¼ê°€ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤:

```json
{
  "pk": "DRIFT#EKS#production-eks",
  "sk": "DRIFT#2024-01-15T10:30:00Z",
  "type": "resource_drift",
  "resource_type": "EKS",
  "resource_id": "production-eks",
  "severity": "HIGH",
  "drifted_fields": [...],

  // ë¶„ì„ ê²°ê³¼ í•„ë“œ
  "analysis_cause_category": "manual_change",
  "analysis_root_cause": "AWS Consoleì—ì„œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì´ ìˆ˜ë™ ë³€ê²½ë¨",
  "analysis_confidence": 0.85,
  "analysis_urgency": 0.7,
  "analysis_requires_review": true,
  "analysis_remediations": [...]
}
```

### ì‚¬ìš© ì˜ˆì‹œ

```python
from src.agents.drift.services.drift_analyzer import DriftAnalyzer
from src.common.services.llm_client import LLMClient, LLMProvider

# LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
llm_client = LLMClient(
    provider=LLMProvider.VLLM,
    endpoint="http://localhost:8000",
    model_name="mistral-7b"
)

# ë¶„ì„ê¸° ìƒì„±
analyzer = DriftAnalyzer(
    llm_client=llm_client,
    max_iterations=3,
    confidence_threshold=0.7
)

# ë“œë¦¬í”„íŠ¸ ë¶„ì„
analysis = analyzer.analyze_drift(
    drift_result=drift,
    baseline_config=baseline,
    current_config=current,
    resource_context={
        "resource_arn": "arn:aws:eks:...",
        "region": "ap-northeast-2"
    }
)

print(f"ì›ì¸: {analysis.cause_analysis.category}")
print(f"ì‹ ë¢°ë„: {analysis.confidence_score}")
print(f"ì¡°ì¹˜: {analysis.remediations}")
```

---

## Human-in-the-Loop (HITL) í†µí•©

### ê°œìš”

Drift AgentëŠ” Streamlit ê¸°ë°˜ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ Human-in-the-Loop(HITL) ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
ì‚¬ìš©ìëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ìš”ì²­í•˜ê³ , ë³µêµ¬ ì‘ì—…ì— ëŒ€í•œ ìŠ¹ì¸/ìˆ˜ì •/ê±°ë¶€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### HITL ì•„í‚¤í…ì²˜

```mermaid
flowchart TB
    subgraph ui["Streamlit UI"]
        user["ğŸ‘¤ ì‚¬ìš©ì"]
        chat["ğŸ’¬ Chat Interface"]
        approval["âœ… Approval Dialog"]
    end

    subgraph chatAgent["ChatAgent (LangGraph)"]
        router["Router"]
        tools["Drift Tools"]
        humanReview["Human Review Node"]
        respond["Respond Node"]
    end

    subgraph driftAgent["Drift Agent"]
        analyzer["DriftAnalyzer<br/>(ReAct)"]
        detector["DriftDetector"]
    end

    subgraph llm["LLM Service"]
        vllm["vLLM / Gemini"]
    end

    user -->|"ë“œë¦¬í”„íŠ¸ ë¶„ì„ ìš”ì²­"| chat
    chat --> router
    router -->|"analyze_config_drift"| tools
    tools --> detector --> analyzer
    analyzer <-->|"ReAct Loop"| vllm
    analyzer -->|"requires_human_review=true"| humanReview
    humanReview -->|"ìŠ¹ì¸ ëŒ€ê¸°"| approval
    approval -->|"APPROVED/MODIFIED/REJECTED"| humanReview
    humanReview --> respond
    respond --> chat
    chat --> user
```

### Streamlit Chat í†µí•©

ChatAgentëŠ” ë“œë¦¬í”„íŠ¸ ë¶„ì„ ë„êµ¬ë¥¼ ì œê³µí•˜ì—¬ ëŒ€í™”í˜• ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ì§€ì›í•©ë‹ˆë‹¤.

#### ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

| ë„êµ¬ | ì„¤ëª… | HITL ì—¬ë¶€ |
|-----|------|----------|
| `analyze_config_drift` | ë“œë¦¬í”„íŠ¸ ê°ì§€ ë° LLM ì›ì¸ ë¶„ì„ | âœ… (ë³µêµ¬ ìŠ¹ì¸ ì‹œ) |
| `check_drift_status` | ë“œë¦¬í”„íŠ¸ ìƒíƒœ ì¡°íšŒ | âŒ |
| `get_remediation_plan` | ë³µêµ¬ ê³„íš ì¡°íšŒ | âŒ |
| `approve_remediation` | ë³µêµ¬ ì‘ì—… ìŠ¹ì¸/ê±°ë¶€ | âœ… |

#### ëŒ€í™” ì˜ˆì‹œ

```
ğŸ‘¤ User: production-eks í´ëŸ¬ìŠ¤í„°ì˜ ë“œë¦¬í”„íŠ¸ ìƒíƒœë¥¼ í™•ì¸í•´ì¤˜

ğŸ¤– Assistant: EKS í´ëŸ¬ìŠ¤í„° ë“œë¦¬í”„íŠ¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.

[ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼]
- ë¦¬ì†ŒìŠ¤: EKS/production-eks
- ë“œë¦¬í”„íŠ¸ ê°ì§€ë¨: âœ…
- ìµœëŒ€ ì‹¬ê°ë„: HIGH
- ë³€ê²½ëœ í•„ë“œ:
  - instance_types: ["m6i.xlarge"] â†’ ["m5.large"] (HIGH)
  - desired_size: 5 â†’ 3 (MEDIUM)

[LLM ì›ì¸ ë¶„ì„]
- ì›ì¸ ì¹´í…Œê³ ë¦¬: MANUAL_CHANGE
- ê·¼ë³¸ ì›ì¸: AWS Consoleì—ì„œ ë¹„ìš© ì ˆê° ëª©ì ìœ¼ë¡œ ìˆ˜ë™ ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ì¶”ì •
- ì‹ ë¢°ë„: 0.85
- ê¸´ê¸‰ë„: 0.7

âš ï¸ ë³µêµ¬ ì‘ì—…ì— ëŒ€í•œ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

ê¶Œì¥ ì¡°ì¹˜:
1. [revert_to_baseline] Terraformìœ¼ë¡œ ê¸°ì¤€ì„  ë³µì›
   ëª…ë ¹: terraform apply -target=module.eks

ì–´ë–»ê²Œ ì§„í–‰í• ê¹Œìš”?
[ìŠ¹ì¸] [ìˆ˜ì • í›„ ìŠ¹ì¸] [ê±°ë¶€] [ì¶”ê°€ ë¶„ì„ ìš”ì²­]
```

### HITL ìŠ¹ì¸ í”Œë¡œìš°

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ ì‚¬ìš©ì
    participant Chat as ğŸ’¬ ChatAgent
    participant Drift as ğŸ” DriftAnalyzer
    participant HITL as âœ… HumanReview

    User->>Chat: "ë“œë¦¬í”„íŠ¸ ë¶„ì„ ìš”ì²­"
    Chat->>Drift: analyze_config_drift()
    Drift-->>Chat: DriftAnalysisResult<br/>(requires_human_review=true)
    Chat->>HITL: ìŠ¹ì¸ ëŒ€ê¸° ìƒíƒœ ì „í™˜
    HITL-->>User: ìŠ¹ì¸ ìš”ì²­ í‘œì‹œ

    alt ìŠ¹ì¸ (APPROVED)
        User->>HITL: "ìŠ¹ì¸"
        HITL->>Chat: approval_status=APPROVED
        Chat->>Drift: execute_remediation()
        Drift-->>Chat: ë³µêµ¬ ì™„ë£Œ
        Chat-->>User: "ë³µêµ¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
    else ìˆ˜ì • í›„ ìŠ¹ì¸ (MODIFIED)
        User->>HITL: "ìˆ˜ì • í›„ ìŠ¹ì¸" + ìˆ˜ì • íŒŒë¼ë¯¸í„°
        HITL->>Chat: approval_status=MODIFIED
        Chat->>Drift: execute_remediation(modified_params)
        Drift-->>Chat: ìˆ˜ì •ëœ ë³µêµ¬ ì™„ë£Œ
        Chat-->>User: "ìˆ˜ì •ëœ ë³µêµ¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
    else ê±°ë¶€ (REJECTED)
        User->>HITL: "ê±°ë¶€" + ì‚¬ìœ 
        HITL->>Chat: approval_status=REJECTED
        Chat-->>User: "ë³µêµ¬ê°€ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë“œë¦¬í”„íŠ¸ê°€ ì˜ë„ì  ë³€ê²½ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."
    end
```

### ìŠ¹ì¸ ìƒíƒœ

| ìƒíƒœ | ì„¤ëª… | í›„ì† ì²˜ë¦¬ |
|-----|------|----------|
| `PENDING` | ì‚¬ìš©ì ìŠ¹ì¸ ëŒ€ê¸° | UIì— ìŠ¹ì¸ ë‹¤ì´ì–¼ë¡œê·¸ í‘œì‹œ |
| `APPROVED` | ë³µêµ¬ ìŠ¹ì¸ë¨ | ë³µêµ¬ ì‘ì—… ì‹¤í–‰ |
| `MODIFIED` | ìˆ˜ì • í›„ ìŠ¹ì¸ë¨ | ìˆ˜ì •ëœ íŒŒë¼ë¯¸í„°ë¡œ ë³µêµ¬ ì‹¤í–‰ |
| `REJECTED` | ë³µêµ¬ ê±°ë¶€ë¨ | ë“œë¦¬í”„íŠ¸ë¥¼ ì˜ë„ì  ë³€ê²½ìœ¼ë¡œ ê¸°ë¡ |

### ChatAgent ë„êµ¬ ë“±ë¡

```python
from src.common.chat.tools import create_chat_tools
from src.common.services.llm_client import LLMClient, LLMProvider

# LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
llm_client = LLMClient(provider=LLMProvider.VLLM)

# ë“œë¦¬í”„íŠ¸ ë„êµ¬ í¬í•¨ ì „ì²´ ë„êµ¬ ì„¸íŠ¸ ìƒì„±
tools = create_chat_tools(
    aws_client=aws_client,
    rds_client=rds_client,
    llm_client=llm_client,  # ë“œë¦¬í”„íŠ¸ ë¶„ì„ìš© LLM
)

# ChatAgent ìƒì„±
agent = ChatAgent(
    llm_client=llm_client,
    tools=tools,
)

# ëŒ€í™” ì‹¤í–‰
response = agent.chat("production-eks ë“œë¦¬í”„íŠ¸ ë¶„ì„í•´ì¤˜")
```

### ë“œë¦¬í”„íŠ¸ ë„êµ¬ ì§ì ‘ ì‚¬ìš©

```python
from src.common.chat.tools.drift import (
    analyze_config_drift,
    check_drift_status,
    get_remediation_plan,
    approve_remediation,
)

# ë“œë¦¬í”„íŠ¸ ë¶„ì„
result = analyze_config_drift(
    baseline_config=baseline,
    current_config=current,
    resource_type="EKS",
    resource_id="production-eks",
    include_analysis=True,  # LLM ë¶„ì„ í¬í•¨
)

# ë¶„ì„ ê²°ê³¼ í™•ì¸
if result["requires_approval"]:
    print(f"ìŠ¹ì¸ í•„ìš”: {result['approval_context']['reason']}")

    # ì‚¬ìš©ì ìŠ¹ì¸ ì²˜ë¦¬
    approval = approve_remediation(
        drift_id="EKS:production-eks",
        action_type="revert_to_baseline",
        approval_status="APPROVED",
        user_feedback="í™•ì¸ í›„ ìŠ¹ì¸í•©ë‹ˆë‹¤.",
    )

    print(f"ìŠ¹ì¸ ê²°ê³¼: {approval['message']}")
```

### HITL í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ëª… | ì„¤ëª… | ê¸°ë³¸ê°’ |
|--------|------|--------|
| `DRIFT_HITL_ENABLED` | HITL ì›Œí¬í”Œë¡œìš° í™œì„±í™” | `true` |
| `DRIFT_AUTO_APPROVE_LOW` | LOW ì‹¬ê°ë„ ìë™ ìŠ¹ì¸ | `false` |
| `DRIFT_APPROVAL_TIMEOUT` | ìŠ¹ì¸ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (ì´ˆ) | `3600` |
| `STREAMLIT_DRIFT_ENABLED` | Streamlitì—ì„œ ë“œë¦¬í”„íŠ¸ ë„êµ¬ í™œì„±í™” | `true` |

---

## ë“œë¦¬í”„íŠ¸ íƒì§€ ì•Œê³ ë¦¬ì¦˜

### íƒì§€ íë¦„

```mermaid
flowchart TB
    baseline["Baseline JSON<br/>(conf/baselines/)"]
    current["Current Config<br/>(AWS API)"]

    baseline --> normalize
    current --> normalize

    normalize["1. Normalize JSON<br/>- Sort keys<br/>- Flatten nested<br/>- Type coercion"]
    normalize --> compare

    compare["2. Deep Compare<br/>- Field by field<br/>- Array comparison<br/>- Nested objects"]
    compare --> added & modified & removed

    added["ADDED<br/>(fields)"]
    modified["MODIFIED<br/>(fields)"]
    removed["REMOVED<br/>(fields)"]

    added & modified & removed --> tolerance

    tolerance["3. Apply Tolerance<br/>- Ignore minor diffs<br/>- Exclude patterns"]
    tolerance --> classify

    classify["4. Classify Severity"]
```

### 1. JSON ì •ê·œí™” (Normalize)

ë¹„êµ ì „ ì–‘ìª½ JSONì„ ì •ê·œí™”í•©ë‹ˆë‹¤.

```python
def normalize_config(config: dict) -> dict:
    """JSON êµ¬ì„±ì„ ë¹„êµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì •ê·œí™”."""
    # 1. í‚¤ ì •ë ¬
    # 2. ì¤‘ì²© ê°ì²´ í‰íƒ„í™” (optional)
    # 3. íƒ€ì… ë³€í™˜ (string â†’ number ë“±)
    # 4. ë¬´ì‹œí•  í•„ë“œ ì œê±° (ì˜ˆ: íƒ€ì„ìŠ¤íƒ¬í”„, ARN ë“±)
    pass
```

### 2. Deep Compare (í•„ë“œë³„ ë¹„êµ)

```python
def compare_configs(baseline: dict, current: dict) -> DriftResult:
    """ë‘ êµ¬ì„±ì„ ë¹„êµí•˜ì—¬ ë“œë¦¬í”„íŠ¸ ê²°ê³¼ ë°˜í™˜."""
    added = []      # ê¸°ì¤€ì„ ì— ì—†ê³  í˜„ì¬ì— ìˆëŠ” í•„ë“œ
    modified = []   # ê°’ì´ ë³€ê²½ëœ í•„ë“œ
    removed = []    # ê¸°ì¤€ì„ ì— ìˆê³  í˜„ì¬ì— ì—†ëŠ” í•„ë“œ

    for key in all_keys:
        if key not in baseline:
            added.append(key)
        elif key not in current:
            removed.append(key)
        elif baseline[key] != current[key]:
            modified.append({
                "field": key,
                "baseline_value": baseline[key],
                "current_value": current[key]
            })

    return DriftResult(added, modified, removed)
```

### 3. í—ˆìš© ë²”ìœ„ (Tolerance) ì„¤ì •

ì¼ë¶€ ë³€ê²½ì€ í—ˆìš© ê°€ëŠ¥í•œ ë²”ìœ„ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
tolerance_rules = {
    "scaling_config.desired_size": {
        "type": "percentage",
        "threshold": 20  # 20% ì´ë‚´ ë³€ê²½ì€ ë¬´ì‹œ
    },
    "tags.*": {
        "type": "ignore"  # íƒœê·¸ ë³€ê²½ì€ LOWë¡œ ë¶„ë¥˜
    },
    "last_modified_*": {
        "type": "ignore"  # íƒ€ì„ìŠ¤íƒ¬í”„ í•„ë“œ ë¬´ì‹œ
    }
}
```

### 4. ì‹¬ê°ë„ ë¶„ë¥˜ ë§¤íŠ¸ë¦­ìŠ¤

| ì‹¬ê°ë„ | ë¶„ë¥˜ ê¸°ì¤€ | ì˜ˆì‹œ |
|--------|----------|------|
| **CRITICAL** | ë³´ì•ˆ ì„¤ì • ì•½í™” | ì•”í˜¸í™” ë¹„í™œì„±í™”, í¼ë¸”ë¦­ ì•¡ì„¸ìŠ¤ í—ˆìš© |
| **HIGH** | ì£¼ìš” ìŠ¤í™ ë³€ê²½ | ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë³€ê²½, ë²„ì „ ë‹¤ìš´ê·¸ë ˆì´ë“œ, ë…¸ë“œ 50%+ ê°ì†Œ |
| **MEDIUM** | ìŠ¤ì¼€ì¼ë§/ìš©ëŸ‰ ë³€ê²½ | min/max ë…¸ë“œ ìˆ˜ ë³€ê²½, ìŠ¤í† ë¦¬ì§€ í¬ê¸° ë³€ê²½ |
| **LOW** | ë©”íƒ€ë°ì´í„° ë³€ê²½ | íƒœê·¸ ë³€ê²½, ë¡œê¹… ì„¤ì • ë³€ê²½ |

#### í•„ë“œë³„ ì‹¬ê°ë„ ë§¤í•‘

```python
SEVERITY_MAPPING = {
    # CRITICAL - ë³´ì•ˆ ê´€ë ¨
    "encryption_config": "CRITICAL",
    "endpoint_public_access": "CRITICAL",
    "public_access_block": "CRITICAL",
    "encryption_at_rest": "CRITICAL",
    "encryption_in_transit": "CRITICAL",

    # HIGH - ì£¼ìš” ìŠ¤í™
    "version": "HIGH",
    "release_label": "HIGH",
    "kafka_version": "HIGH",
    "airflow_version": "HIGH",
    "instance_type": "HIGH",
    "instance_types": "HIGH",
    "environment_class": "HIGH",
    "number_of_broker_nodes": "HIGH",

    # MEDIUM - ìŠ¤ì¼€ì¼ë§/ìš©ëŸ‰
    "scaling_config": "MEDIUM",
    "min_size": "MEDIUM",
    "max_size": "MEDIUM",
    "min_workers": "MEDIUM",
    "max_workers": "MEDIUM",
    "volume_size": "MEDIUM",
    "disk_size": "MEDIUM",
    "storage_info": "MEDIUM",

    # LOW - ë©”íƒ€ë°ì´í„°
    "tags": "LOW",
    "logging": "LOW",
    "logging_configuration": "LOW",
}
```

---

## ê¸°ì¤€ì„  ê´€ë¦¬

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

ê¸°ì¤€ì„  íŒŒì¼ì€ agentì˜ `conf/baselines/` ë””ë ‰í† ë¦¬ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.

```
conf/
â””â”€â”€ baselines/
    â”œâ”€â”€ eks/
    â”‚   â”œâ”€â”€ production-cluster.json
    â”‚   â”œâ”€â”€ staging-cluster.json
    â”‚   â””â”€â”€ _schema.json
    â”œâ”€â”€ msk/
    â”‚   â”œâ”€â”€ production-kafka.json
    â”‚   â””â”€â”€ _schema.json
    â”œâ”€â”€ s3/
    â”‚   â”œâ”€â”€ data-lake-bucket.json
    â”‚   â”œâ”€â”€ logs-bucket.json
    â”‚   â””â”€â”€ _schema.json
    â”œâ”€â”€ emr/
    â”‚   â”œâ”€â”€ analytics-cluster.json
    â”‚   â””â”€â”€ _schema.json
    â””â”€â”€ mwaa/
        â”œâ”€â”€ orchestration-env.json
        â””â”€â”€ _schema.json
```

### Baseline Loader

```python
import json
from pathlib import Path
from typing import Optional


class BaselineLoader:
    """ë¡œì»¬ ê¸°ì¤€ì„  íŒŒì¼ ë¡œë”."""

    def __init__(self, baselines_dir: str = "conf/baselines"):
        self.baselines_dir = Path(baselines_dir)

    def get_baseline(
        self,
        resource_type: str,
        resource_id: str
    ) -> Optional[dict]:
        """ê¸°ì¤€ì„  JSON íŒŒì¼ ë¡œë“œ."""
        # ë¦¬ì†ŒìŠ¤ íƒ€ì…ë³„ ë””ë ‰í† ë¦¬
        type_dir = self.baselines_dir / resource_type.lower()

        # íŒŒì¼ëª… íŒ¨í„´: {resource_id}.json
        baseline_file = type_dir / f"{resource_id}.json"

        if not baseline_file.exists():
            return None

        with open(baseline_file, "r", encoding="utf-8") as f:
            return json.load(f)

    def list_baselines(self, resource_type: str) -> list[str]:
        """íŠ¹ì • ë¦¬ì†ŒìŠ¤ íƒ€ì…ì˜ ëª¨ë“  ê¸°ì¤€ì„  íŒŒì¼ ëª©ë¡ ë°˜í™˜."""
        type_dir = self.baselines_dir / resource_type.lower()

        if not type_dir.exists():
            return []

        return [
            f.stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
            for f in type_dir.glob("*.json")
            if not f.name.startswith("_")  # _schema.json ë“± ì œì™¸
        ]

    def get_baseline_hash(self, resource_type: str, resource_id: str) -> Optional[str]:
        """ê¸°ì¤€ì„  íŒŒì¼ì˜ í•´ì‹œê°’ ë°˜í™˜ (ë³€ê²½ ì¶”ì ìš©)."""
        import hashlib

        baseline_file = self.baselines_dir / resource_type.lower() / f"{resource_id}.json"

        if not baseline_file.exists():
            return None

        with open(baseline_file, "rb") as f:
            return hashlib.sha256(f.read()).hexdigest()[:12]
```

### ê¸°ì¤€ì„  íŒŒì¼ ëª…ëª… ê·œì¹™

| ë¦¬ì†ŒìŠ¤ íƒ€ì… | íŒŒì¼ëª… íŒ¨í„´ | ì˜ˆì‹œ |
|------------|------------|------|
| EKS | `{cluster-name}.json` | `production-eks.json` |
| MSK | `{cluster-name}.json` | `production-kafka.json` |
| S3 | `{bucket-name}.json` | `data-lake-prod.json` |
| EMR | `{cluster-name}.json` | `analytics-emr-prod.json` |
| MWAA | `{environment-name}.json` | `bdp-airflow-prod.json` |

---

## ì§€ì› ë¦¬ì†ŒìŠ¤

### EKS (Elastic Kubernetes Service)

#### AWS API

```python
# í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ
eks_client.describe_cluster(name="cluster-name")

# ë…¸ë“œê·¸ë£¹ ì •ë³´ ì¡°íšŒ
eks_client.describe_nodegroup(
    clusterName="cluster-name",
    nodegroupName="nodegroup-name"
)
```

#### ê¸°ì¤€ì„  ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ

```json
{
  "cluster_name": "production-eks",
  "version": "1.29",
  "endpoint_public_access": false,
  "endpoint_private_access": true,
  "logging": {
    "api": true,
    "audit": true,
    "authenticator": true,
    "controllerManager": true,
    "scheduler": true
  },
  "node_groups": [
    {
      "name": "general-workload",
      "instance_types": ["m6i.xlarge", "m6i.2xlarge"],
      "scaling_config": {
        "min_size": 3,
        "max_size": 10,
        "desired_size": 5
      },
      "disk_size": 100,
      "ami_type": "AL2_x86_64",
      "capacity_type": "ON_DEMAND"
    }
  ],
  "tags": {
    "Environment": "production",
    "ManagedBy": "cd1-agent"
  }
}
```

### MSK (Managed Streaming for Apache Kafka)

#### AWS API

```python
# í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ
kafka_client.describe_cluster_v2(ClusterArn="arn:...")
```

#### ê¸°ì¤€ì„  ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ

```json
{
  "cluster_name": "production-kafka",
  "kafka_version": "3.5.1",
  "broker_config": {
    "instance_type": "kafka.m5.large",
    "number_of_broker_nodes": 3,
    "storage_info": {
      "ebs_storage_info": {
        "volume_size": 1000,
        "provisioned_throughput": {
          "enabled": true,
          "volume_throughput": 250
        }
      }
    }
  },
  "encryption_info": {
    "encryption_at_rest": true,
    "encryption_in_transit": "TLS"
  },
  "enhanced_monitoring": "PER_TOPIC_PER_BROKER",
  "open_monitoring": {
    "prometheus": {
      "jmx_exporter": true,
      "node_exporter": true
    }
  },
  "tags": {
    "Environment": "production"
  }
}
```

### S3 (Simple Storage Service)

#### AWS API

```python
# ë²„í‚· ì •ë³´ ì¡°íšŒ
s3_client.get_bucket_versioning(Bucket="bucket-name")
s3_client.get_bucket_encryption(Bucket="bucket-name")
s3_client.get_public_access_block(Bucket="bucket-name")
s3_client.get_bucket_lifecycle_configuration(Bucket="bucket-name")
```

#### ê¸°ì¤€ì„  ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ

```json
{
  "bucket_name": "company-data-lake-prod",
  "versioning": {
    "status": "Enabled"
  },
  "encryption": {
    "sse_algorithm": "aws:kms",
    "kms_master_key_id": "alias/data-lake-key",
    "bucket_key_enabled": true
  },
  "public_access_block": {
    "block_public_acls": true,
    "ignore_public_acls": true,
    "block_public_policy": true,
    "restrict_public_buckets": true
  },
  "lifecycle_rules": [
    {
      "id": "archive-old-data",
      "status": "Enabled",
      "transitions": [
        { "days": 90, "storage_class": "STANDARD_IA" },
        { "days": 365, "storage_class": "GLACIER" }
      ],
      "expiration_days": 2555
    }
  ],
  "logging": {
    "enabled": true,
    "target_bucket": "company-access-logs",
    "target_prefix": "s3/data-lake/"
  },
  "tags": {
    "Environment": "production",
    "DataClassification": "confidential"
  }
}
```

### EMR (Elastic MapReduce)

#### AWS API

```python
# í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¡°íšŒ
emr_client.describe_cluster(ClusterId="j-XXXXX")
emr_client.list_instance_groups(ClusterId="j-XXXXX")
```

#### ê¸°ì¤€ì„  ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ

```json
{
  "cluster_name": "analytics-emr-prod",
  "release_label": "emr-7.0.0",
  "applications": ["Spark", "Hadoop", "Hive"],
  "instance_groups": {
    "master": {
      "instance_type": "m5.xlarge",
      "instance_count": 1,
      "ebs_config": {
        "volume_type": "gp3",
        "volume_size": 100,
        "volumes_per_instance": 1
      }
    },
    "core": {
      "instance_type": "r5.2xlarge",
      "instance_count": 4,
      "ebs_config": {
        "volume_type": "gp3",
        "volume_size": 500,
        "volumes_per_instance": 2
      }
    },
    "task": {
      "instance_type": "r5.2xlarge",
      "instance_count": 0,
      "market": "SPOT",
      "bid_price": "0.50"
    }
  },
  "auto_scaling_policy": {
    "min_capacity": 4,
    "max_capacity": 20
  },
  "configurations": [
    {
      "classification": "spark-defaults",
      "properties": {
        "spark.executor.memory": "8g",
        "spark.driver.memory": "4g"
      }
    }
  ],
  "tags": {
    "Environment": "production"
  }
}
```

### MWAA (Managed Workflows for Apache Airflow)

#### AWS API

```python
# í™˜ê²½ ì •ë³´ ì¡°íšŒ
mwaa_client.get_environment(Name="environment-name")
```

#### ê¸°ì¤€ì„  ìŠ¤í‚¤ë§ˆ ì˜ˆì‹œ

```json
{
  "environment_name": "bdp-airflow-prod",
  "airflow_version": "2.8.1",
  "environment_class": "mw1.medium",
  "min_workers": 2,
  "max_workers": 10,
  "schedulers": 2,
  "webserver_access_mode": "PRIVATE_ONLY",
  "weekly_maintenance_window_start": "SUN:03:00",
  "airflow_configuration_options": {
    "core.default_timezone": "Asia/Seoul",
    "core.parallelism": "32",
    "core.dag_concurrency": "16"
  },
  "logging_configuration": {
    "dag_processing_logs": {
      "enabled": true,
      "log_level": "INFO"
    },
    "scheduler_logs": {
      "enabled": true,
      "log_level": "WARNING"
    },
    "task_logs": {
      "enabled": true,
      "log_level": "INFO"
    },
    "webserver_logs": {
      "enabled": true,
      "log_level": "WARNING"
    },
    "worker_logs": {
      "enabled": true,
      "log_level": "INFO"
    }
  },
  "tags": {
    "Environment": "production"
  }
}
```

---

## í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ëª… | ì„¤ëª… | ê¸°ë³¸ê°’ |
|--------|------|--------|
| `AWS_MOCK` | Mock ëª¨ë“œ í™œì„±í™” | `false` |
| `BASELINES_DIR` | ê¸°ì¤€ì„  íŒŒì¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ | `conf/baselines` |
| `CONFIG_DRIFT_TABLE` | ë“œë¦¬í”„íŠ¸ ì´ë ¥ í…Œì´ë¸” | `bdp-config-drift-tracking` |
| `EVENT_BUS_NAME` | EventBridge ë²„ìŠ¤ ì´ë¦„ | `default` |

### ë¦¬ì†ŒìŠ¤ë³„ í™˜ê²½ ë³€ìˆ˜

```bash
# EKS
export EKS_CLUSTERS='["production-eks", "staging-eks"]'

# MSK
export MSK_CLUSTER_ARNS='["arn:aws:kafka:..."]'

# S3
export S3_BUCKETS='["data-lake-prod", "logs-prod"]'

# EMR
export EMR_CLUSTER_IDS='["j-XXXXX"]'

# MWAA
export MWAA_ENVIRONMENTS='["bdp-airflow-prod"]'
```

---

## DynamoDB í…Œì´ë¸”

### bdp-config-drift-tracking

ë“œë¦¬í”„íŠ¸ ì´ë ¥ ì¶”ì  í…Œì´ë¸”.

| ì†ì„± | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| `resource_id` (PK) | String | AWS ë¦¬ì†ŒìŠ¤ ARN ë˜ëŠ” ì´ë¦„ |
| `detected_at` (SK) | String | íƒì§€ ì‹œê°„ (ISO 8601) |
| `resource_type` | String | EKS / MSK / S3 / EMR / MWAA |
| `drift_type` | String | ADDED / MODIFIED / REMOVED |
| `drifted_fields` | List | ë“œë¦¬í”„íŠ¸ ë°œìƒ í•„ë“œ ëª©ë¡ |
| `baseline_hash` | String | ê¸°ì¤€ì„  íŒŒì¼ í•´ì‹œ (SHA256 ì• 12ìë¦¬) |
| `baseline_values` | Map | ê¸°ì¤€ì„  ê°’ |
| `current_values` | Map | í˜„ì¬ ê°’ |
| `severity` | String | CRITICAL / HIGH / MEDIUM / LOW |
| `resolved` | Boolean | í•´ê²° ì—¬ë¶€ |
| `resolved_at` | String | í•´ê²° ì‹œê°„ |
| `ttl` | Number | TTL (90ì¼ í›„ ìë™ ì‚­ì œ) |

**GSI**:
- `resource-type-index`: ë¦¬ì†ŒìŠ¤ íƒ€ì…ë³„ ì¡°íšŒ
- `severity-date-index`: ì‹¬ê°ë„ë³„ ì¡°íšŒ
- `unresolved-index`: ë¯¸í•´ê²° ë“œë¦¬í”„íŠ¸ ì¡°íšŒ

---

## EventBridge ì´ë²¤íŠ¸

### ì´ë²¤íŠ¸ íƒ€ì…

| DetailType | ì¡°ê±´ | ëŒ€ìƒ |
|------------|------|------|
| `CONFIG_DRIFT_CRITICAL` | severity=CRITICAL | Slack + Email + PagerDuty |
| `CONFIG_DRIFT_HIGH` | severity=HIGH | Slack + Email |
| `CONFIG_DRIFT_MEDIUM` | severity=MEDIUM | Slack |
| `CONFIG_DRIFT_LOW` | severity=LOW | ë¡œê·¸ ê¸°ë¡ |

### ì´ë²¤íŠ¸ êµ¬ì¡°

```json
{
  "version": "0",
  "source": "bdp.config-drift",
  "detail-type": "CONFIG_DRIFT_HIGH",
  "detail": {
    "drift_id": "eks-production-cluster-2024-01-15-abc123",
    "resource_type": "EKS",
    "resource_id": "production-eks",
    "resource_arn": "arn:aws:eks:ap-northeast-2:123456789012:cluster/production-eks",
    "severity": "HIGH",
    "drift_summary": {
      "added_fields": 0,
      "modified_fields": 2,
      "removed_fields": 0
    },
    "drifted_fields": [
      {
        "field": "node_groups[0].instance_types",
        "baseline_value": ["m6i.xlarge"],
        "current_value": ["m5.large"],
        "severity": "HIGH"
      },
      {
        "field": "node_groups[0].scaling_config.desired_size",
        "baseline_value": 5,
        "current_value": 3,
        "severity": "MEDIUM"
      }
    ],
    "baseline_hash": "abc123def456",
    "baseline_file": "conf/baselines/eks/production-eks.json",
    "detected_at": "2024-01-15T10:30:00Z"
  }
}
```

### EventBridge Rule ì˜ˆì‹œ

```json
{
  "source": ["bdp.config-drift"],
  "detail-type": ["CONFIG_DRIFT_CRITICAL", "CONFIG_DRIFT_HIGH"],
  "detail": {
    "resource_type": ["EKS", "MSK"]
  }
}
```

---

## ì‚¬ìš©ë²•

### Lambda í˜¸ì¶œ

```python
import boto3
import json

lambda_client = boto3.client('lambda')

response = lambda_client.invoke(
    FunctionName='bdp-drift-detection',
    InvocationType='RequestResponse',
    Payload=json.dumps({
        "resource_types": ["EKS", "MSK", "S3"],
        "resources": {
            "EKS": ["production-eks"],
            "MSK": ["arn:aws:kafka:..."],
            "S3": ["data-lake-prod"]
        },
        "severity_threshold": "MEDIUM"
    })
)

result = json.loads(response['Payload'].read())
print(result)
```

### ì‘ë‹µ ì˜ˆì‹œ

```json
{
  "statusCode": 200,
  "body": {
    "drifts_detected": true,
    "total_drift_count": 3,
    "severity_summary": {
      "CRITICAL": 0,
      "HIGH": 1,
      "MEDIUM": 2,
      "LOW": 0
    },
    "resources_analyzed": 5,
    "drift_details": [
      {
        "resource_type": "EKS",
        "resource_id": "production-eks",
        "severity": "HIGH",
        "drifted_fields": [
          {
            "field": "node_groups[0].instance_types",
            "baseline": ["m6i.xlarge"],
            "current": ["m5.large"]
          }
        ]
      }
    ],
    "baseline_info": {
      "baselines_dir": "conf/baselines",
      "baseline_hash": "abc123def456"
    },
    "execution_time_ms": 3250
  }
}
```

### Airflow DAG ì˜ˆì‹œ

```python
from airflow import DAG
from airflow.providers.amazon.aws.operators.lambda_function import LambdaInvokeFunctionOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'bdp-team',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'bdp_config_drift_detection',
    default_args=default_args,
    description='Configuration Drift Detection DAG',
    schedule_interval='0 9 * * *',  # ë§¤ì¼ 09:00 KST
    start_date=datetime(2024, 1, 1),
    catchup=False,
) as dag:

    detect_eks_drift = LambdaInvokeFunctionOperator(
        task_id='detect_eks_drift',
        function_name='bdp-drift-detection',
        payload=json.dumps({
            "resource_types": ["EKS"],
            "severity_threshold": "MEDIUM"
        }),
    )

    detect_msk_drift = LambdaInvokeFunctionOperator(
        task_id='detect_msk_drift',
        function_name='bdp-drift-detection',
        payload=json.dumps({
            "resource_types": ["MSK"],
            "severity_threshold": "MEDIUM"
        }),
    )

    [detect_eks_drift, detect_msk_drift]
```

---

## Mock í…ŒìŠ¤íŠ¸

### í™˜ê²½ ì„¤ì •

```bash
export AWS_MOCK=true
```

### Baseline Loader í…ŒìŠ¤íŠ¸

```python
from src.agents.drift.services.baseline_loader import BaselineLoader

# ë¡œì»¬ ê¸°ì¤€ì„  ë¡œë” ìƒì„±
loader = BaselineLoader(baselines_dir="conf/baselines")

# ê¸°ì¤€ì„  ì¡°íšŒ
baseline = loader.get_baseline(
    resource_type="EKS",
    resource_id="production-eks"
)
print(baseline)

# ê¸°ì¤€ì„  ëª©ë¡ ì¡°íšŒ
baselines = loader.list_baselines(resource_type="EKS")
print(f"EKS baselines: {baselines}")

# ê¸°ì¤€ì„  í•´ì‹œ ì¡°íšŒ
hash_value = loader.get_baseline_hash(
    resource_type="EKS",
    resource_id="production-eks"
)
print(f"Baseline hash: {hash_value}")
```

### ë“œë¦¬í”„íŠ¸ ì£¼ì… í…ŒìŠ¤íŠ¸

```python
from src.agents.drift.services.config_fetcher import ConfigFetcher, ConfigProvider
from src.agents.drift.services.drift_detector import ConfigDriftDetector
from src.agents.drift.services.baseline_loader import BaselineLoader

# Mock Fetcher ìƒì„±
fetcher = ConfigFetcher(provider=ConfigProvider.MOCK)

# í˜„ì¬ êµ¬ì„± ì¡°íšŒ (Mock ë°ì´í„° ì‚¬ìš©)
current_config = fetcher.get_config(
    resource_type="EKS",
    resource_id="production-eks"
)

# ê¸°ì¤€ì„  ë¡œë“œ
loader = BaselineLoader()
baseline = loader.get_baseline(
    resource_type="EKS",
    resource_id="production-eks"
)

# ë“œë¦¬í”„íŠ¸ íƒì§€
detector = ConfigDriftDetector()
result = detector.detect(
    baseline=baseline,
    current=current_config.config,
    resource_type="EKS",
    resource_id="production-eks"
)

print(f"Drift detected: {result.has_drift}")
print(f"Severity: {result.max_severity}")
for field in result.drifted_fields:
    print(f"  - {field.field_path}: {field.baseline_value} â†’ {field.current_value}")
```

### Handler í†µí•© í…ŒìŠ¤íŠ¸

```bash
python -c "
import os
os.environ['AWS_MOCK'] = 'true'

from src.agents.drift.handler import handler as lambda_handler
import json

class MockContext:
    aws_request_id = 'test-123'

result = lambda_handler({
    'resource_types': ['EKS', 'MSK'],
    'severity_threshold': 'LOW'
}, MockContext())

print(json.dumps(json.loads(result['body']), indent=2))
"
```

---

## ê´€ë ¨ ë¬¸ì„œ

- [Architecture Guide](ARCHITECTURE.md) - ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
- [HDSP Detection](HDSP_DETECTION.md) - On-Prem K8s ì¥ì•  ê°ì§€ (HDSP Agent)
- [Cost Anomaly Detection](COST_ANOMALY_DETECTION.md) - ë¹„ìš© ì´ìƒ íƒì§€ (Cost Agent)

## ì°¸ê³ 

- [AWS EKS API Reference](https://docs.aws.amazon.com/eks/latest/APIReference/)
- [AWS MSK API Reference](https://docs.aws.amazon.com/msk/latest/APIReference/)
- [AWS S3 API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/)
- [AWS EMR API Reference](https://docs.aws.amazon.com/emr/latest/APIReference/)
- [AWS MWAA API Reference](https://docs.aws.amazon.com/mwaa/latest/API/)
